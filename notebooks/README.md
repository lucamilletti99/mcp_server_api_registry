# API Registry Agent Notebook

## ðŸŽ¯ Purpose

This notebook demonstrates the **recommended Databricks pattern** for building agentic applications with Foundation Models and custom tools.

## ðŸ—ï¸ Architecture Comparison

### Current FastAPI Approach (Complex)
```
User Browser
    â†“ (HTTP)
Frontend React App
    â†“ (HTTP POST /api/chat/message)
FastAPI Chat Router
    â†“ (HTTP POST to serving endpoint)
Databricks Foundation Model
    â†“ (returns tool_calls)
Frontend parses response
    â†“ (HTTP POST /api/chat/execute-tool)
FastAPI executes MCP tools
    â†“ (HTTP back)
Frontend sends results back
    â†“ (HTTP POST /api/chat/message again)
Foundation Model generates final response
```

**Problems:**
- ðŸŒ 5+ network hops per query
- ðŸ”¥ Complex error handling at each layer
- ðŸ“¦ Serialization/deserialization overhead
- ðŸ› Hard to debug
- ðŸ“Š No native MLflow tracing
- ðŸ’” Tool results need careful type handling

### Notebook Approach (Recommended)
```
Notebook Cell
    â†“ (Direct Python call)
run_agent(user_query)
    â†“ (Python function call)
call_foundation_model()
    â†“ (1 HTTP call to serving endpoint)
Foundation Model returns tool_calls
    â†“ (Python function call)
execute_tool()
    â†“ (Direct function execution - NO HTTP!)
Tool returns Python dict
    â†“ (Send back to model)
Foundation Model generates response
    âœ… All traced in MLflow automatically!
```

**Benefits:**
- âš¡ 1-2 network hops per query (only model serving calls)
- ðŸŽ¯ Direct Python function calls for tools
- ðŸ” Full MLflow tracing out of the box
- ðŸ› Easy to debug (add print statements anywhere)
- ðŸ“Š See step-by-step reasoning in notebook cells
- ðŸš€ Production-ready pattern
- ðŸŽ¨ Beautiful visualization in Databricks UI

## ðŸš€ How to Use

### 1. Open the Notebook in Databricks

Navigate to your Databricks workspace:
- Go to **Workspace** â†’ **Users** â†’ **[your-email]**
- Click on `api_registry_agent.py`

### 2. Run the Setup Cells

Execute the first few cells to:
- Install dependencies
- Enable MLflow tracing
- Define tools and schemas

### 3. Try the Examples

The notebook includes three example queries:
1. **Simple query**: Check API registry health
2. **Complex query**: Multi-tool orchestration
3. **Discovery**: Search for specific APIs

### 4. View MLflow Traces

After running queries:
1. Click the **Experiments** icon in the sidebar
2. Select the latest run
3. View **Traces** tab to see:
   - Complete conversation flow
   - Each tool execution
   - Timing information
   - Input/output at every step

This gives you the "Databricks Playground" experience!

## ðŸ“Š MLflow Tracing Benefits

Every agent interaction is automatically logged:

```python
# MLflow captures:
- User query input
- Model calls (with latency)
- Tool executions (with parameters and results)
- Final response
- Total tokens used
- Error traces (if any)
```

You can:
- Compare different model responses
- Debug tool execution failures
- Optimize agent performance
- Monitor production usage
- Create dashboards

## ðŸ”§ Customizing Tools

To add your own tools:

1. **Define the Python function:**
```python
def my_custom_tool(param1: str, param2: int) -> Dict[str, Any]:
    """Tool description for the model."""
    # Your logic here
    return {"result": "data"}
```

2. **Add to tool registry:**
```python
TOOL_REGISTRY["my_custom_tool"] = my_custom_tool
```

3. **Define the schema:**
```python
TOOLS.append({
    "type": "function",
    "function": {
        "name": "my_custom_tool",
        "description": "What the tool does",
        "parameters": {
            "type": "object",
            "properties": {
                "param1": {"type": "string", "description": "First parameter"},
                "param2": {"type": "integer", "description": "Second parameter"}
            },
            "required": ["param1"]
        }
    }
})
```

4. **Test it:**
```python
result = await run_agent("Use my custom tool to...")
```

## ðŸŽ¯ When to Use This vs FastAPI

| Use Case | Use Notebook | Use FastAPI App |
|----------|-------------|-----------------|
| Interactive analysis | âœ… | âŒ |
| Demos & presentations | âœ… | âŒ |
| Development & debugging | âœ… | âŒ |
| MLflow experiment tracking | âœ… | âš ï¸ (harder) |
| Claude CLI integration | âŒ | âœ… |
| Public web interface | âŒ | âœ… |
| Scheduled jobs | âœ… | âœ… (both work) |
| Production APIs for external clients | âŒ | âœ… |

## ðŸ”— Integration with FastAPI

You can **use both approaches** together:

1. **Keep FastAPI for:**
   - MCP server endpoints (for Claude CLI)
   - Public web UI
   - Admin dashboard

2. **Use Notebook for:**
   - Core agentic logic
   - Tool development
   - MLflow tracing
   - Interactive demos

3. **Import notebook code in FastAPI:**
```python
# In server/routers/chat.py
from notebooks.api_registry_agent import run_agent, TOOLS

@router.post('/agent/query')
async def agent_query(request: AgentRequest):
    # Use the notebook's agentic logic!
    result = await run_agent(request.query, model=request.model)
    return result
```

This gives you the best of both worlds:
- Fast, direct tool execution (notebook pattern)
- Web interface for users (FastAPI)
- Full MLflow tracing (notebook pattern)
- Claude CLI integration (FastAPI MCP server)

## ðŸ“š Resources

- [Databricks Foundation Models](https://docs.databricks.com/en/machine-learning/foundation-models/index.html)
- [Building AI Agents with Foundation Models](https://docs.databricks.com/en/generative-ai/tutorials/ai-agents.html)
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)
- [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html)

## ðŸŽ“ Key Takeaways

1. **Direct Python > HTTP** for tool execution
2. **MLflow tracing** gives you observability for free
3. **Notebook pattern** is recommended by Databricks for agents
4. **FastAPI + Notebook** can work together beautifully
5. **Step-by-step visibility** makes debugging 10x easier

## ðŸš€ Next Steps

1. Run the notebook and see the traces in MLflow
2. Add your own custom tools
3. Try different Foundation Models (Claude 4, Llama 3.3, etc.)
4. Export the agentic logic to a production API endpoint
5. Set up monitoring dashboards in MLflow

**Have fun building agents! ðŸ¤–**
